---
title: "Online Retail detailed EDA"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers. 
Online retail data set is extracted from University of California, Irwin machine learning repository. Data set is also present on www.kaggle.com/jihyeseo/online-retail-data-set-from-uci-ml-repo.

The purpose of the data analysis is to capture the insights with the business data, which can be used in making the business oriented decisions. The major part of the analysis consist of exploratory data analysis, data wrangling as they are basic and important steps for data science projects. 

The motive here is to analyze the sales happening on monthly, daily and hourly basis. On the basis of which we can further think of predicting the future sales. This online data is of ocassional gifts ordered by various customers across various countries.


 


## Data Preparations

Data prepration is important to make sure we are going forward with the right data and donot mess up when deriving the conclusion.

Let's take a dive and figure out what are various features pressent, various anomalies in the data set.
Starting by loading library "tidyverse", loading the online retail data and checking the summary of the data.
```{r Load libraries}
# loading library tidyverse
library(tidyverse)
library(lubridate)

# load the data
data <- read_csv("online_retail.csv")

```

## Data set Column	Description

1.  InvoiceNo	Invoice- number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.
2. StockCode	Product (item) code- 5-digit integral number uniquely assigned to each distinct product.
3. Description	Product (item) name- Nominal.
4. Quantity- 	The quantities of each product (item) per transaction. 
5. InvoiceDate-	The day and time when each transaction was generated.
6. UnitPrice-	Product price per unit in sterling.
7. CustomerID- Nominal, a 5-digit integral number uniquely assigned to each customer.
8. Country- The name of the country where each customer resides.


```{r}
# summary of the data
summary(data)

```

In a single glance it is clear that there is something not right with Quantity and Unit Price. Let's check for NA values.
Looking at the data rows and columns
```{r}
#how many rows are present
nrow(data) 

#how many columns
ncol(data)

#look at the first 6 rows
head(data)

```
Checking for NA values 

```{r}
# number of NA vlaues
data %>% is.na %>% colSums
```
CustomerID variable doesn't seems to be necessary can be removed as there are n mumber of customers and there is nothing much to evaluate from them. NA values are present in Discription also.
We are not removing NA values of Descriptions there are many more categories of missing values and Unit Price  of them is 0. So, all the unwanted values in description will be gone once we values of UnitPrice= 0. As they   will not contribute in calculating sales.

## Data Cleaning 

Removing free orders(means Unit price = 0) and returned orders by eliminating negative UnitPrice.

Check the summary of data
```{r}

# remove free orders
data <- data %>%
  filter(UnitPrice != 0)

#removing the CustomerID
data<- data %>% select(-c(CustomerID))

#summary
summary(data)
```
Negative values of Quantity are removed and Customer Id variable is removed from the df

Checking for NA values 
```{r}
# number of NA vlaues
data %>% is.na %>% colSums
```
all the NA values are removed.


## Data Pre-processing-1 for single variate plot.
Pre-processing of data is required for single variable EDA to figure out the outliers, not required categories in order to calcualte the total sales over the period of time.

```{r}
######### Pre processing-1 of data

#Transform description and country column as factor 
data <- data %>%mutate(Description = as.factor(Description),
                       Country = as.factor(Country))

# Hourly, daily and monthly split of date time for Invoice date will be used in single and multivariable plot. 
data$InvoiceDate <- mdy_hm(data$InvoiceDate)
data$date <- format(data$InvoiceDate, "%m/%d/%Y")
data$month <- format(data$InvoiceDate, "%B")
data$week <- format(data$InvoiceDate, "%A")
data$time <- format(data$InvoiceDate, "%H")

#total sales
data$Total_sales = data$Quantity * data$UnitPrice

#summary of data
head(data)

```
Seems like we have sufficient data for single variable analysis to find out the outliers, unwanted values.


## Single variable EDA
1. Defining the customer base across the countries

```{r}
# Customer base by countries
ggplot(data=data, aes(x=Country))+
  geom_bar()+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))+
  scale_y_continuous(labels = scales::format_format(scientific = FALSE))+
  ggtitle("UK shares the major customer base")+
  ylab("frequency based on orders")
```

The above graphs displays that UK has the major portion of the customers with respect to other countries. Let's see the plot without the UK customer base



2.  Defining the customer base across the countries(without UK)

```{r}
# Customer base by country without UK
p1 <- data %>% filter (Country != "United Kingdom") %>%
  ggplot(aes(x=Country))+
  geom_bar()+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))+
  ggtitle("Customer base without UK")+
  ylab("frequency based on orders")
p1
```

The above graphs displays that Germany, France and Ireland are top 3 countries where online retail is working but it's very low in comparison to UK. So, we should be focusing on UK only. Removal of all other countries will be done in pre-processing-2


3. Single variable plot for most sold Product Stock Code

```{r}
# extracting the products with the help of stockcode
#top 10 products
p3 <- data %>% group_by(Description, StockCode) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  head(10)

# plot
p3 %>% ggplot(aes(x= Description, y= count))+
  geom_bar(stat = 'identity', fill = "darkgreen") +
  coord_flip() + theme_classic()+ 
  xlab("Products")+
  ylab("Number of products sold")+
  ggtitle("Products most bought by Customers")
```

Above graph represents the top 10 products which are mostly ordered by users.


4. Single variable plot in which hours the maxixmum orders are placed
```{r}
# what time most products are sold
ggplot(data= data, aes(x=time))+
  geom_histogram(stat = "count", fill = "lightcoral")+ 
  theme_classic()+ 
  xlab("Hours through out the day")+
  ylab("Count of Orders")+
  ggtitle("Most customer buy products between 1000 hrs to 1500hrs")
```

The above graph explains that between morning 10 am till 3pm most of the orders are placed on the online portal(from every country).

5. Single variable plot for Cancelled orders

It's time to dig a little deeper and dig into analysis of Cancelled orders. 
```{r}
#Seperating cancelled, Adjusted and Regular orders
data <- data %>% mutate(InvoicePrefix = ifelse(substr(InvoiceNo,1,1) %in% letters | substr(InvoiceNo,1,1) %in% LETTERS, substr(InvoiceNo,1,1), "R"))

#plot
ggplot(data = data, aes(x=InvoicePrefix))+
  geom_bar(position = "dodge", fill = "#08306b")+
  scale_y_continuous(labels = scales::format_format(big.mark = ".", decimal.mark = ",", scientific = FALSE))+ labs(x = "Invoice type (Adjusted, Cancelled and Regular)", y = "Number of invoices")

```

It can be inferred from the graph that, there are significant number of cancelled orders. Let's check what's happening with the cancelled orders and how it is impacting the Total Sales. 


## Data pre-processing-2 for multivariate EDA
Let's remove the outliers present and investigate the errors stepwise
1. What all products contibute to top 20 sales
```{r}
#Step-1
#products contribute most to sales
top20_product_sales <- data %>%
  select(StockCode, Description, Total_sales)%>%
  group_by(StockCode, Description, Total_sales) %>%
  filter(rank(desc(Total_sales)) <= 20) %>%
  arrange(desc(Total_sales)) %>%
  head(20)
head(top20_product_sales, 20)
```

These seems to be like bad adjustments, Amazon fee, Postage, Manual, Dotcom Postage are not the actual contributors to the sale. We'll be removing the observations relating to them. But first checking the relation of cancelled orders with top grossing sales by comparing the exact total sales amount.  

```{r}
#Step-2
# Selecting top total sales with respective invoices tocheck for cancelled orders
p4 <- data %>%
  select(InvoiceNo, StockCode, Description, Quantity, Total_sales, InvoicePrefix)%>%
  filter(abs(Total_sales) %in% c(168469.60, 77183.60, 38970.00, 13541.33, 11062.06, 8142.75, 7144.72, 6539.40, 4992.00, 4921.50, 4781.60, 4632.00, 4522.50, 4401.00, 4287.63, 4254.50, 4176.00, 4161.06)) %>%
  arrange(desc(abs(Total_sales)))
p4

```

Ah.Okay! We can clearly see that the top 3 earning highest total sales are cancelled orders rest of them are majorly cancelled, adjust bad debts, Amazon fee, Manual, Postage fee. Let's remove the Stockcode which donot contribute to Total sales. Remove particular Invoices which are cancelled and rest of them looks fine as there is no relation to it.

Cleaning the unwanted orders and top cancelled orders
```{r}
# Step-3
## removing all the products wrongly contributing to sales
data <- data %>%
  filter(!StockCode %in% c("DOT", "POST", "M", "AMAZONFEE", "B"))

# Specific Invoices are deleted
data <- data[!(data$InvoiceNo=="581483"),]
data <- data[!(data$InvoiceNo=="C581484"),]
data <- data[!(data$InvoiceNo=="541431"),]
data <- data[!(data$InvoiceNo=="C541433"),]
data <- data[!(data$InvoiceNo=="556444"),]
data <- data[!(data$InvoiceNo=="540815"),]
data <- data[!(data$InvoiceNo=="C550456"),]
data <- data[!(data$InvoiceNo=="C550456"),]
data <- data[!(data$InvoiceNo=="C550456"),]

# again checking the top products
#products contribute most to sales
top20_product_sales <- data %>%
  select(StockCode, Description, Total_sales)%>%
  group_by(StockCode, Description, Total_sales) %>%
  filter(rank(desc(Total_sales)) <= 20) %>%
  arrange(desc(Total_sales)) %>%
  head(20)
head(top20_product_sales, 20)


```

Now this seems to be legit total sales data from which multivariate analysis will be done monthly, daily and hourly.

Removing the variables which will not be used further, considering only UK data for further analysis.
```{r}
# Step-4
# Filtering only UK data
data <- data %>% filter(Country == "United Kingdom")

# removing the variables which will not be needed
data<- data %>% select(-c(Country, Description, StockCode, InvoiceDate, InvoicePrefix))

head(data)


```

All the unwanted variables are gone and only UK data is present.


## Multi variable EDA
1. Plotting the total sales on monthly basis
```{r}
# Multiple variable EDA

# calculating the total sales by month 
data2 <- data %>%
  group_by(month)%>%
  summarise(Total_sales=sum(Total_sales))%>%
  arrange(match(month, month.name))

#Line plot
ggplot(data=data2,aes(x=ordered(month, month.name), y=Total_sales, group =1))+ggtitle("Total Sales by Month ")+
  geom_line(colour = "maroon", size = 1.2)+
  geom_point(color="darkblue")+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),plot.title = element_text(hjust=0.5, lineheight = 0.8, face = "bold"))+
  ylab("Total Sales") + xlab("Months of the Year")


```

The above graph shows that from September to December the sale is high in comparison to other months of the year. November is the peak season. Reason can be due to fall festivals like (christmas,  New year, Thanksgiving, Halloween). Gift purchasing is on the rise.


2. Plotting the total sales on daily basis

```{r}
# Multiple variable EDA

# calculating total sales on daily basis
daywise <- data %>%
  group_by(week)%>%
  summarise(Total_sales=sum(Total_sales))%>%
  arrange(week)

#reorder the table according to Weekdays, so that when it comes to data visualization the weekdays will be in the right order
daywise$week<- factor(daywise$week,
                      levels=c("Sunday","Monday","Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))

daywise<-daywise[order(daywise$week),]

#Line plot
ggplot(data=daywise,aes(x=week, y=Total_sales, group =1))+ggtitle("Total Sales by Day")+
  geom_line(colour = "skyblue", size = 1.2)+
  geom_point(color="darkblue")+
  theme(plot.title = element_text(hjust=0.5, lineheight = .8, face = "bold"))+
  ylab("Total Sales") + xlab("Days of the week")


``` 

The above graph shows that Tuesday's and Thursday's are the days where more Total Sale is happening in comparison to other weekdays. Saturday seems to be off for orders.

3. Plotting the total sales on hourly basis

```{r}
# Multiple variable EDA

# calculating total sales on hourly basis
hourly<- data %>%
  group_by(time)%>%
  summarise(Total_sales=sum(Total_sales))

#line plot
ggplot(data=hourly,aes(x=time, y=Total_sales, group =1))+ggtitle("Trend of Sales by hour")+
  geom_line(colour = "orange", size = 1.2)+
  geom_point(color="dark green")+
  theme(plot.title = element_text(hjust=0.5, lineheight = .8, face = "bold"))+
  ylab("Total Sales") + xlab("Hours")


``` 

The above line plot gives the clear picture that 1000hrs to 1500hrs generate the most of sales during the day. This implicates the behaviour of the customers, major section id active during the office hours.


From the above analysis of single variable and multi variable EDA, basic focus is to identify how the sale is happening over the time. The perfect idea we get due to busy delivery system at the time of holidays, people tend to order in November for most of the holidays. The steep increase in sales in November can also be due to the sale season. Special sale for holidays is always considered as one of the best ideas. 





